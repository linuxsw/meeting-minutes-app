# Save this code as audio_processor.py
import argparse
import json
import os
import sys
import tempfile
import warnings

# Suppress Hugging Face and other warnings for cleaner output in a production-like script
warnings.filterwarnings("ignore")
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "0" # Ensure online mode for model downloads if not cached

# Add a try-except block for SpeechBrain import to provide a clearer error if it's not installed
try:
    from speechbrain.pretrained import EncoderClassifier, SpeakerDiarization, EncoderDecoderASR
    from speechbrain.dataio.dataio import read_audio_info, read_audio
    import torchaudio
    import torch
except ImportError as e:
    print(f"Error: A required library is not installed. Please ensure SpeechBrain, PyTorch, and Torchaudio are correctly installed. Details: {e}", file=sys.stderr)
    sys.exit(1)

def process_audio(audio_file_path, language="auto", output_sample_rate=16000):
    # 1. Load audio and resample if necessary
    try:
        waveform, sample_rate = torchaudio.load(audio_file_path)
        if waveform.ndim > 1 and waveform.shape[0] > 1: # Check if stereo
            waveform = torch.mean(waveform, dim=0, keepdim=True) # Convert to mono by averaging channels
        if sample_rate != output_sample_rate:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=output_sample_rate)
            waveform = resampler(waveform)
        input_duration_seconds = waveform.shape[1] / output_sample_rate
    except Exception as e:
        return {"error": f"Failed to load or preprocess audio: {str(e)}"}

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 2. Language Identification (LID) - Skip if language is specified
    detected_language = language
    if language == "auto":
        try:
            language_id = EncoderClassifier.from_hparams(
                source="speechbrain/lang-id-commonlanguage_ecapa", 
                savedir="pretrained_models/lang-id-commonlanguage_ecapa"
            )
            language_id.to(device)
            waveform_lid = waveform.to(device) # Ensure waveform is on the correct device
            out_prob = language_id.classify_batch(waveform_lid)
            scores = out_prob[0]
            predicted_index = torch.argmax(scores)
            detected_language = language_id.hparams.label_encoder.decode_torch(predicted_index.unsqueeze(0))[0]
        except Exception as e:
            # Log language detection failure but proceed with a default (e.g., English)
            print(f"Language detection failed: {str(e)}. Defaulting to English.", file=sys.stderr)
            detected_language = "en"  # Default to English if auto-detection fails
    else:
        print(f"Using specified language: {language}", file=sys.stderr)

    # 3. Automatic Speech Recognition (ASR)
    # Choose model based on detected or specified language
    asr_model_source = "speechbrain/asr-wav2vec2-commonvoice-en"  # Default English model
    
    # Map language codes to appropriate ASR models
    language_model_map = {
        "en": "speechbrain/asr-wav2vec2-commonvoice-en",
        "ko": "speechbrain/asr-wav2vec2-commonvoice-en",  # Replace with Korean model when available
        "ja": "speechbrain/asr-wav2vec2-commonvoice-en",  # Replace with Japanese model when available
        "zh": "speechbrain/asr-wav2vec2-commonvoice-en",  # Replace with Chinese model when available
        # Add more language-specific models as needed
    }
    
    # Select the appropriate model based on detected language
    if detected_language in language_model_map:
        asr_model_source = language_model_map[detected_language]
    else:
        print(f"No specific model for {detected_language}, using default English model", file=sys.stderr)
    
    full_transcript = ""
    try:
        asr_model = EncoderDecoderASR.from_hparams(
            source=asr_model_source, 
            savedir=f"pretrained_models/{asr_model_source.split('/')[-1]}"
        )
        asr_model.to(device)
        waveform_asr = waveform.to(device) # Ensure waveform is on the correct device
        # Transcribe the whole audio to get a full transcript for context
        # Note: For long audio, batching or streaming ASR would be more robust
        transcription_result = asr_model.transcribe_batch(waveform_asr)
        full_transcript = transcription_result[0][0] # Get the transcribed text
    except Exception as e:
        return {"error": f"Transcription failed: {str(e)}", "language": detected_language}

    # 4. Speaker Diarization
    # This uses a pre-trained diarization model. It might require specific audio properties.
    # The output of diarization is typically a list of segments with speaker labels and timestamps.
    speaker_segments = []
    try:
        # Using a more complete diarization pipeline from SpeechBrain
        # This model performs VAD, speaker embeddings, and clustering.
        diarization_model = SpeakerDiarization.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb", # Base model for embeddings
            savedir="pretrained_models/spkrec-ecapa-voxceleb",
            run_opts={"device": str(device)} # Ensure model runs on the correct device
        )
        # The diarize_file method expects a file path. We can save the processed waveform to a temp file.
        # Or, if the model supports waveform input directly, that's preferable.
        # For SpeechBrain's SpeakerDiarization, it often works with file paths or can be adapted.
        # Let's try to use the waveform directly if possible, or save to a temporary file.

        # Create a temporary WAV file from the processed waveform for the diarizer
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_audio_file:
            torchaudio.save(tmp_audio_file.name, waveform, output_sample_rate)
            tmp_audio_file_path = tmp_audio_file.name
        
        # Perform diarization
        # diarization_model.diarize_file might not be the direct method for all SpeechBrain diarization setups.
        # Often, it involves calling specific methods for VAD, embeddings, clustering, etc.
        # The example `diarize_file` is a high-level abstraction that might exist in some pipelines.
        # For this example, we'll assume a simplified output format or use a placeholder approach
        # if a direct `diarize_file` method isn't straightforward with this base model.

        # A more robust approach would be to use a dedicated diarization pipeline like:
        # from speechbrain.pipelines import DiarizationPipeline
        # pipeline = DiarizationPipeline.from_hparams(source="...", savedir="...")
        # boundaries = pipeline(tmp_audio_file_path)
        # However, SpeechBrain's direct diarization pipelines might vary.
        # For now, we'll simulate a basic segmentation based on the full transcript, 
        # as robust diarization is complex and model-dependent.

        # Placeholder: If full diarization is too complex for this step, 
        # we can split the transcript into sentences and assign speakers round-robin or based on VAD.
        # This is NOT real diarization but a step towards it.
        # For now, let's assume the diarization model gives us speaker boundaries.
        # Example output from a diarization model: RTTM-like format or list of (speaker, start, end)
        # boundaries = diarization_model.diarize_file(tmp_audio_file_path) # This is a conceptual call
        # The line above is conceptual. Actual SpeechBrain diarization might be more involved.
        # For example, using `speechbrain.nnet.speaker_verification.diarize_multi_channel` or similar.

        # Let's use the `diarize_file` method which is part of the SpeakerDiarization class
        # This method performs VAD, speaker embeddings, spectral clustering, and speaker counting.
        # It returns speaker labels for segments. We need to map these to text.
        boundaries = diarization_model.diarize_file(tmp_audio_file_path) # This should work with the class
        # The `boundaries` tensor contains speaker labels for frames. We need to convert this to segments.
        # This part requires careful processing of the `boundaries` output to create meaningful segments.
        # Each segment should have a speaker, start_time, end_time, and corresponding text.
        # This alignment is non-trivial.

        # For now, as a step towards full diarization, let's process the boundaries:
        # The `boundaries` tensor is (1, num_frames, num_speakers) with probabilities or one-hot labels.
        # We need to convert this frame-level information into time segments.
        # This is a simplified interpretation of `boundaries` for demonstration.
        # A real implementation would need to parse `boundaries` according to the model's specific output format.
        # (e.g., by finding contiguous blocks of frames for each speaker).

        # Clean up the temporary audio file
        os.unlink(tmp_audio_file_path)

        # This is a simplified way to create segments from the boundaries. 
        # A more accurate method would involve looking at changes in speaker labels in the 'boundaries' tensor.
        # For now, we'll create segments based on the diarization output if it's in a usable format.
        # The output of `diarize_file` is usually frame-level speaker labels. 
        # We need to convert this to (speaker, start, end) segments.
        # This is a complex step. Let's assume `boundaries` gives us segments directly for now, or we use a placeholder.

        # Placeholder for processing `boundaries` into speaker_segments
        # This part needs to be implemented based on the actual output format of `diarization_model.diarize_file`
        # For now, we will split the full transcript somewhat arbitrarily if proper diarization is not available.
        # This is a placeholder and needs to be replaced with actual diarization logic.
        if boundaries is not None and hasattr(boundaries, 'tolist'): # Check if boundaries are somewhat usable
            # This is a very simplified interpretation and likely needs refinement
            # based on the actual structure of `boundaries` from SpeechBrain.
            # Assuming `boundaries` can be converted to a list of speaker changes or segments.
            # For now, we'll just use the full transcript and assign speakers based on some logic.
            # This is a placeholder for actual diarization result processing.
            # A real diarization would give segments like: [('spk_0', 0.0, 10.5), ('spk_1', 10.5, 15.0), ...]
            # And then you'd need to map transcript parts to these segments.
            current_speaker_id = 0
            num_words = len(full_transcript.split())
            words_per_segment = max(10, num_words // max(1, boundaries.shape[-1] if boundaries.ndim ==3 else 2)) # Heuristic
            words = full_transcript.split()
            current_segment_text = []
            start_time_approx = 0.0
            segment_duration_approx = input_duration_seconds / max(1, (num_words / words_per_segment))

            for i, word in enumerate(words):
                current_segment_text.append(word)
                if (i + 1) % words_per_segment == 0 or (i + 1) == num_words:
                    segment_text = " ".join(current_segment_text)
                    end_time_approx = start_time_approx + segment_duration_approx * (len(current_segment_text) / words_per_segment)
                    speaker_segments.append({
                        "speaker": f"Speaker {current_speaker_id + 1}",
                        "start_time": round(start_time_approx, 2),
                        "end_time": round(min(end_time_approx, input_duration_seconds), 2),
                        "text": segment_text
                    })
                    current_segment_text = []
                    start_time_approx = end_time_approx
                    current_speaker_id = (current_speaker_id + 1) % (boundaries.shape[-1] if boundaries.ndim ==3 else 2) # Cycle through speakers
            if not speaker_segments: # Fallback if loop didn't run
                 speaker_segments.append({"speaker": "Speaker 1", "start_time": 0, "end_time": round(input_duration_seconds,2), "text": full_transcript})

        else: # Fallback if diarization fails or boundaries are not usable
            speaker_segments.append({"speaker": "Speaker 1", "start_time": 0, "end_time": round(input_duration_seconds,2), "text": full_transcript})

    except Exception as e:
        print(f"Speaker diarization failed: {str(e)}. Falling back to single speaker.", file=sys.stderr)
        # Fallback to single speaker if diarization fails
        speaker_segments = [
            {
                "speaker": "Speaker 1", 
                "start_time": 0,
                "end_time": round(input_duration_seconds, 2),
                "text": full_transcript
            }
        ]

    return {
        "language": detected_language,
        "segments": speaker_segments,
        "full_transcript_debug": full_transcript # For debugging, can be removed later
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process audio file for transcription and speaker diarization.")
    parser.add_argument("audio_file", help="Path to the audio file to process.")
    parser.add_argument("--language", default="auto", help="Language code (e.g., 'en', 'ko', 'ja') or 'auto' for automatic detection.")
    args = parser.parse_args()

    if not os.path.exists(args.audio_file):
        print(json.dumps({"error": f"Audio file not found: {args.audio_file}"}), file=sys.stderr)
        sys.exit(1)

    # Create a temporary directory for SpeechBrain models if it doesn't exist
    temp_model_dir = os.path.join(tempfile.gettempdir(), "sb_models")
    os.makedirs(temp_model_dir, exist_ok=True)

    result = process_audio(args.audio_file, args.language)
    
    print(json.dumps(result, indent=2)) # Added indent for readability
